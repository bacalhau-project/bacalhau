---
sidebar_label: "Sparkov-Data-Generation"
sidebar_position: 2
---
# Generate Synthetic Data using Sparkov Data Generation technique

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bacalhau-project/examples/blob/main/workload-onboarding/Sparkov-Data-Generation/index.ipynb)
[![Open In Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/bacalhau-project/examples/HEAD?labpath=workload-onboarding/Sparkov-Data-Generation/index.ipynb)

## Introduction

A synthetic dataset is generated by algorithms or simulations which has similar characteristics of real world data. Collecting real world data especially the data which contains sensitive user data like credit card information is not possible due to security and privacy concerns. If a data scientist needs to train  a model to detect credit fraud they can use synthetically generated data instead of using the real data without compromising privacy of users.

The advantage of using bacalhau is that you can generate terabytes of synthetic data without having to install any dependencies or store the data locally. 

In this example we will generate synthetic credit card transaction data using the Sparkov program and store the results to IPFS

## Prerequisite 

- The Bacalhau client - [Installation instructions](https://docs.bacalhau.org/getting-started/installation)

## Running Sparkov Locallyâ€‹

To run Sparkov locally, you'll need to clone the repo and install dependencies.



```bash
%%bash
git clone https://github.com/js-ts/Sparkov_Data_Generation/
pip3 install -r Sparkov_Data_Generation/requirements.txt
```

    Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
    Collecting Faker==13.12.0
      Downloading Faker-13.12.0-py3-none-any.whl (1.6 MB)
    Requirement already satisfied: numpy==1.21.6 in /usr/local/lib/python3.7/dist-packages (from -r Sparkov_Data_Generation/requirements.txt (line 2)) (1.21.6)
    Requirement already satisfied: typing-extensions>=3.10.0.2 in /usr/local/lib/python3.7/dist-packages (from Faker==13.12.0->-r Sparkov_Data_Generation/requirements.txt (line 1)) (4.1.1)
    Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.7/dist-packages (from Faker==13.12.0->-r Sparkov_Data_Generation/requirements.txt (line 1)) (2.8.2)
    Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.4->Faker==13.12.0->-r Sparkov_Data_Generation/requirements.txt (line 1)) (1.15.0)
    Installing collected packages: Faker
    Successfully installed Faker-13.12.0


    Cloning into 'Sparkov_Data_Generation'...



```python
%cd Sparkov_Data_Generation
```

    /content/Sparkov_Data_Generation


Creating a temporary directory to store the outputs


```bash
%%bash
mkdir ../outputs
```

## Running the script

Below are some of the parameters you need before running the script

- `-n`:  Number of customers to generate

- `-o`: path to store the outputs

- Start date: "01-01-2022" 
 
- End date: "10-01-2022"

To see the full list of options, use:


```bash
%%bash
python datagen.py -h
```

    usage: datagen.py [-h] [-n NB_CUSTOMERS] [-seed [SEED]] [-config [CONFIG]]
                      [-c CUSTOMER_FILE] [-o OUTPUT]
                      start_date end_date
    
    Customer Generator
    
    positional arguments:
      start_date            Transactions start date
      end_date              Transactions start date
    
    optional arguments:
      -h, --help            show this help message and exit
      -n NB_CUSTOMERS, --nb_customers NB_CUSTOMERS
                            Number of customers to generate
      -seed [SEED]          Random generator seed
      -config [CONFIG]      Profile config file (typically
                            profiles/main_config.json")
      -c CUSTOMER_FILE, --customer_file CUSTOMER_FILE
                            Customer file generated with the datagen_customer
                            script
      -o OUTPUT, --output OUTPUT
                            Output Folder path



```bash
%%bash
python3 datagen.py -n 1000 -o ../outputs "01-01-2022" "10-01-2022"
```

    Num CPUs: 2
    profile: adults_50up_male_urban.json, chunk size: 200,                 chunk: 0-199
    profile: adults_50up_male_urban.json, chunk size: 200,                 chunk: 200-399
    profile: adults_50up_male_urban.json, chunk size: 200,                 chunk: 400-599
    profile: adults_50up_male_urban.json, chunk size: 200,                 chunk: 600-799
    profile: adults_50up_male_urban.json, chunk size: 200,                 chunk: 800-999
    profile: adults_50up_female_urban.json, chunk size: 200,                 chunk: 0-199
    profile: adults_50up_female_urban.json, chunk size: 200,                 chunk: 200-399
    profile: adults_50up_female_urban.json, chunk size: 200,                 chunk: 400-599
    profile: adults_50up_female_urban.json, chunk size: 200,                 chunk: 600-799
    profile: adults_50up_female_urban.json, chunk size: 200,                 chunk: 800-999
    profile: adults_50up_male_rural.json, chunk size: 200,                 chunk: 0-199
    profile: adults_50up_male_rural.json, chunk size: 200,                 chunk: 200-399
    profile: adults_50up_male_rural.json, chunk size: 200,                 chunk: 400-599
    profile: adults_50up_male_rural.json, chunk size: 200,                 chunk: 600-799
    profile: adults_50up_male_rural.json, chunk size: 200,                 chunk: 800-999
    profile: adults_50up_female_rural.json, chunk size: 200,                 chunk: 0-199
    profile: adults_50up_female_rural.json, chunk size: 200,                 chunk: 200-399
    profile: adults_50up_female_rural.json, chunk size: 200,                 chunk: 400-599
    profile: adults_50up_female_rural.json, chunk size: 200,                 chunk: 600-799
    profile: adults_50up_female_rural.json, chunk size: 200,                 chunk: 800-999
    profile: adults_2550_male_urban.json, chunk size: 200,                 chunk: 0-199
    profile: adults_2550_male_urban.json, chunk size: 200,                 chunk: 200-399
    profile: adults_2550_male_urban.json, chunk size: 200,                 chunk: 400-599
    profile: adults_2550_male_urban.json, chunk size: 200,                 chunk: 600-799
    profile: adults_2550_male_urban.json, chunk size: 200,                 chunk: 800-999
    profile: adults_2550_female_urban.json, chunk size: 200,                 chunk: 0-199
    profile: adults_2550_female_urban.json, chunk size: 200,                 chunk: 200-399
    profile: adults_2550_female_urban.json, chunk size: 200,                 chunk: 400-599
    profile: adults_2550_female_urban.json, chunk size: 200,                 chunk: 600-799
    profile: adults_2550_female_urban.json, chunk size: 200,                 chunk: 800-999
    profile: adults_2550_male_rural.json, chunk size: 200,                 chunk: 0-199
    profile: adults_2550_male_rural.json, chunk size: 200,                 chunk: 200-399
    profile: adults_2550_male_rural.json, chunk size: 200,                 chunk: 400-599
    profile: adults_2550_male_rural.json, chunk size: 200,                 chunk: 600-799
    profile: adults_2550_male_rural.json, chunk size: 200,                 chunk: 800-999
    profile: adults_2550_female_rural.json, chunk size: 200,                 chunk: 0-199
    profile: adults_2550_female_rural.json, chunk size: 200,                 chunk: 200-399
    profile: adults_2550_female_rural.json, chunk size: 200,                 chunk: 400-599
    profile: adults_2550_female_rural.json, chunk size: 200,                 chunk: 600-799
    profile: adults_2550_female_rural.json, chunk size: 200,                 chunk: 800-999
    profile: young_adults_male_urban.json, chunk size: 200,                 chunk: 0-199
    profile: young_adults_male_urban.json, chunk size: 200,                 chunk: 200-399
    profile: young_adults_male_urban.json, chunk size: 200,                 chunk: 400-599
    profile: young_adults_male_urban.json, chunk size: 200,                 chunk: 600-799
    profile: young_adults_male_urban.json, chunk size: 200,                 chunk: 800-999
    profile: young_adults_female_urban.json, chunk size: 200,                 chunk: 0-199
    profile: young_adults_female_urban.json, chunk size: 200,                 chunk: 200-399
    profile: young_adults_female_urban.json, chunk size: 200,                 chunk: 400-599
    profile: young_adults_female_urban.json, chunk size: 200,                 chunk: 600-799
    profile: young_adults_female_urban.json, chunk size: 200,                 chunk: 800-999
    profile: young_adults_male_rural.json, chunk size: 200,                 chunk: 0-199
    profile: young_adults_male_rural.json, chunk size: 200,                 chunk: 200-399
    profile: young_adults_male_rural.json, chunk size: 200,                 chunk: 400-599
    profile: young_adults_male_rural.json, chunk size: 200,                 chunk: 600-799
    profile: young_adults_male_rural.json, chunk size: 200,                 chunk: 800-999
    profile: young_adults_female_rural.json, chunk size: 200,                 chunk: 0-199
    profile: young_adults_female_rural.json, chunk size: 200,                 chunk: 200-399
    profile: young_adults_female_rural.json, chunk size: 200,                 chunk: 400-599
    profile: young_adults_female_rural.json, chunk size: 200,                 chunk: 600-799
    profile: young_adults_female_rural.json, chunk size: 200,                 chunk: 800-999


## Building a Docker container (Optional)

To use Bacalhau, you need to package your code in an appropriate format. We have already pushed a container for you to use, but if you want to build your own, you can follow the steps below. You can view a [dedicated container example](../custom-containers/index.md) in the documentation.

:::tip
You can skip this section entirely and directly go to running on bacalhau
:::

### Dockerfile

In this step, you will create a `Dockerfile` to create an image. `Dockerfile` is a text document that contains the commands used to assemble the image. 

Copy and paste the following in your `Dockerfile`.

```
FROM python:3.8

RUN apt update && apt install git

RUN git clone https://github.com/js-ts/Sparkov_Data_Generation/

WORKDIR /Sparkov_Data_Generation/

RUN pip3 install -r requirements.txt
```

To build the docker container, run the docker build command

```
docker build -t hub-user/repo-name:tag .
```

Replace the following:

- `&lt;hub-user>`: with your docker hub username, If you donâ€™t have a docker hub account Follow these instructions to create docker account, and use the username of the account you created

- `&lt;repo-name>`: This is the name of the container, you can name it anything you want

- `&lt;tag>`: This is not required but you can use the latest tag

After you have build the container, the next step is to test it locally and then push it docker hub

You'll need to push this repository to the registry designated by its name or tag.

```
 docker push hub-user/repo-name:tag
```


After the repo image has been pushed to docker hub, we can now use the container for running on bacalhau

## Running on Bacalhau

Now we're ready to run a Bacalhau job. This code runs a job, downloads the results and prints the stdout.

Copy and paste the following code to your terminal


```bash
%%bash --out job_id
bacalhau docker run \
--id-only \
--wait \
jsacex/sparkov-data-generation \
--  python3 datagen.py -n 1000 -o ../outputs "01-01-2022" "10-01-2022"
```


```python
%env JOB_ID={job_id}
```

    env: JOB_ID=d986b432-9af6-4463-93d2-362dbccb8379


Running the commands above will output a `UUID` that represents the job that was created. You can check the status of the job with the following command:


```bash
%%bash
bacalhau list --id-filter ${JOB_ID}
```

    [92;100m CREATED  [0m[92;100m ID       [0m[92;100m JOB                     [0m[92;100m STATE     [0m[92;100m VERIFIED [0m[92;100m PUBLISHED               [0m
    [97;40m 12:03:03 [0m[97;40m d986b432 [0m[97;40m Docker jsacex/sparko... [0m[97;40m Completed [0m[97;40m          [0m[97;40m /ipfs/QmQSfVLAZGoy8K... [0m



Where it says "`Completed`", that means the job is done, and we can get the results.

To find out more information about your job, run the following command:


```bash
%%bash
bacalhau describe ${JOB_ID}
```

If you see that the job has completed and there are no errors, then you can download the results with the following command:


```bash
%%bash
rm -rf results && mkdir -p results
bacalhau get $JOB_ID --output-dir results
```

    Fetching results of job 'd986b432-9af6-4463-93d2-362dbccb8379'...
    Results for job 'd986b432-9af6-4463-93d2-362dbccb8379' have been written to...
    results


    2022/11/12 12:05:30 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 2048 kiB, got: 416 kiB). See https://github.com/lucas-clemente/quic-go/wiki/UDP-Receive-Buffer-Size for details.


After the download has finished you should 
see the following contents in results directory


```bash
%%bash
ls results/combined_results/outputs
```

    adults_2550_female_rural_000-199.csv
    adults_2550_female_rural_200-399.csv
    adults_2550_female_rural_400-599.csv
    adults_2550_female_rural_600-799.csv
    adults_2550_female_rural_800-999.csv
    adults_2550_female_urban_000-199.csv
    adults_2550_female_urban_200-399.csv
    adults_2550_female_urban_400-599.csv
    adults_2550_female_urban_600-799.csv
    adults_2550_female_urban_800-999.csv
    adults_2550_male_rural_000-199.csv
    adults_2550_male_rural_200-399.csv
    adults_2550_male_rural_400-599.csv
    adults_2550_male_rural_600-799.csv
    adults_2550_male_rural_800-999.csv
    adults_2550_male_urban_000-199.csv
    adults_2550_male_urban_200-399.csv
    adults_2550_male_urban_400-599.csv
    adults_2550_male_urban_600-799.csv
    adults_2550_male_urban_800-999.csv
    adults_50up_female_rural_000-199.csv
    adults_50up_female_rural_200-399.csv
    adults_50up_female_rural_400-599.csv
    adults_50up_female_rural_600-799.csv
    adults_50up_female_rural_800-999.csv
    adults_50up_female_urban_000-199.csv
    adults_50up_female_urban_200-399.csv
    adults_50up_female_urban_400-599.csv
    adults_50up_female_urban_600-799.csv
    adults_50up_female_urban_800-999.csv
    adults_50up_male_rural_000-199.csv
    adults_50up_male_rural_200-399.csv
    adults_50up_male_rural_400-599.csv
    adults_50up_male_rural_600-799.csv
    adults_50up_male_rural_800-999.csv
    adults_50up_male_urban_000-199.csv
    adults_50up_male_urban_200-399.csv
    adults_50up_male_urban_400-599.csv
    adults_50up_male_urban_600-799.csv
    adults_50up_male_urban_800-999.csv
    customers.csv
    young_adults_female_rural_000-199.csv
    young_adults_female_rural_200-399.csv
    young_adults_female_rural_400-599.csv
    young_adults_female_rural_600-799.csv
    young_adults_female_rural_800-999.csv
    young_adults_female_urban_000-199.csv
    young_adults_female_urban_200-399.csv
    young_adults_female_urban_400-599.csv
    young_adults_female_urban_600-799.csv
    young_adults_female_urban_800-999.csv
    young_adults_male_rural_000-199.csv
    young_adults_male_rural_200-399.csv
    young_adults_male_rural_400-599.csv
    young_adults_male_rural_600-799.csv
    young_adults_male_rural_800-999.csv
    young_adults_male_urban_000-199.csv
    young_adults_male_urban_200-399.csv
    young_adults_male_urban_400-599.csv
    young_adults_male_urban_600-799.csv
    young_adults_male_urban_800-999.csv

