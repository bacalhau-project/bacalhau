# coding: utf-8

from __future__ import absolute_import
from datetime import date, datetime  # noqa: F401

from typing import List, Dict  # noqa: F401

from bacalhau-client.models.base_model_ import Model
from bacalhau-client import util


class ModelJobShardingConfig(Model):
    """NOTE: This class is auto generated by the swagger code generator program.

    Do not edit the class manually.
    """
    def __init__(self, batch_size: int=None, glob_pattern: str=None, glob_pattern_base_path: str=None):  # noqa: E501
        """ModelJobShardingConfig - a model defined in Swagger

        :param batch_size: The batch_size of this ModelJobShardingConfig.  # noqa: E501
        :type batch_size: int
        :param glob_pattern: The glob_pattern of this ModelJobShardingConfig.  # noqa: E501
        :type glob_pattern: str
        :param glob_pattern_base_path: The glob_pattern_base_path of this ModelJobShardingConfig.  # noqa: E501
        :type glob_pattern_base_path: str
        """
        self.swagger_types = {
            'batch_size': int,
            'glob_pattern': str,
            'glob_pattern_base_path': str
        }

        self.attribute_map = {
            'batch_size': 'BatchSize',
            'glob_pattern': 'GlobPattern',
            'glob_pattern_base_path': 'GlobPatternBasePath'
        }
        self._batch_size = batch_size
        self._glob_pattern = glob_pattern
        self._glob_pattern_base_path = glob_pattern_base_path

    @classmethod
    def from_dict(cls, dikt) -> 'ModelJobShardingConfig':
        """Returns the dict as a model

        :param dikt: A dict.
        :type: dict
        :return: The model.JobShardingConfig of this ModelJobShardingConfig.  # noqa: E501
        :rtype: ModelJobShardingConfig
        """
        return util.deserialize_model(dikt, cls)

    @property
    def batch_size(self) -> int:
        """Gets the batch_size of this ModelJobShardingConfig.

        how many \"items\" are to be processed in each shard we first apply the glob pattern which will result in a flat list of items this number decides how to group that flat list into actual shards run by compute nodes  # noqa: E501

        :return: The batch_size of this ModelJobShardingConfig.
        :rtype: int
        """
        return self._batch_size

    @batch_size.setter
    def batch_size(self, batch_size: int):
        """Sets the batch_size of this ModelJobShardingConfig.

        how many \"items\" are to be processed in each shard we first apply the glob pattern which will result in a flat list of items this number decides how to group that flat list into actual shards run by compute nodes  # noqa: E501

        :param batch_size: The batch_size of this ModelJobShardingConfig.
        :type batch_size: int
        """

        self._batch_size = batch_size

    @property
    def glob_pattern(self) -> str:
        """Gets the glob_pattern of this ModelJobShardingConfig.

        divide the inputs up into the smallest possible unit for example /* would mean \"all top level files or folders\" this being an empty string means \"no sharding\"  # noqa: E501

        :return: The glob_pattern of this ModelJobShardingConfig.
        :rtype: str
        """
        return self._glob_pattern

    @glob_pattern.setter
    def glob_pattern(self, glob_pattern: str):
        """Sets the glob_pattern of this ModelJobShardingConfig.

        divide the inputs up into the smallest possible unit for example /* would mean \"all top level files or folders\" this being an empty string means \"no sharding\"  # noqa: E501

        :param glob_pattern: The glob_pattern of this ModelJobShardingConfig.
        :type glob_pattern: str
        """

        self._glob_pattern = glob_pattern

    @property
    def glob_pattern_base_path(self) -> str:
        """Gets the glob_pattern_base_path of this ModelJobShardingConfig.

        when using multiple input volumes what path do we treat as the common mount path to apply the glob pattern to  # noqa: E501

        :return: The glob_pattern_base_path of this ModelJobShardingConfig.
        :rtype: str
        """
        return self._glob_pattern_base_path

    @glob_pattern_base_path.setter
    def glob_pattern_base_path(self, glob_pattern_base_path: str):
        """Sets the glob_pattern_base_path of this ModelJobShardingConfig.

        when using multiple input volumes what path do we treat as the common mount path to apply the glob pattern to  # noqa: E501

        :param glob_pattern_base_path: The glob_pattern_base_path of this ModelJobShardingConfig.
        :type glob_pattern_base_path: str
        """

        self._glob_pattern_base_path = glob_pattern_base_path
