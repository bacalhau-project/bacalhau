name: Publish Binaries to S3

on:
  workflow_call:
    inputs:
      target_type:
        description: 'Target type for publishing (edge, nightly, release, non-release)'
        required: true
        type: string
      environment:
        description: 'Environment for publishing (e.g. release)'
        required: false
        default: 'release'
        type: string
    secrets:
      AWS_ACCESS_KEY_ID:
        required: true
      AWS_SECRET_ACCESS_KEY:
        required: true

jobs:
  publish-to-s3:
    name: Publish to S3
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}  # Use the specified environment
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: "0"  # Fetch all history for proper versioning

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: bacalhau-*
          path: artifacts

      - name: List all downloaded artifacts
        run: |
          echo "==== LISTING ALL DOWNLOADED ARTIFACTS ===="
          find artifacts -type f | sort
          echo "==== ARTIFACT DIRECTORIES ===="
          find artifacts -type d | sort
          echo "==== TOTAL FILE COUNT ===="
          find artifacts -type f | wc -l
          echo "==== TOTAL DIRECTORY COUNT ===="
          find artifacts -type d | wc -l

      - name: Get version info
        id: version
        uses: ./.github/actions/get-version-info

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Upload binaries to S3
        id: upload
        run: |
          # Determine S3 prefix based on target type
          case "${{ inputs.target_type }}" in
            edge)
              S3_PREFIX="edge"
              ;;
            nightly)
              S3_PREFIX="nightly"
              ;;
            release)
              if [[ "${{ steps.version.outputs.release_type }}" == "release" ]]; then
                S3_PREFIX="stable"
              else
                S3_PREFIX="pre"
              fi
              ;;
            *)
              echo "Unknown target type: ${{ inputs.target_type }}"
              exit 1
              ;;
          esac
          
          echo "Publishing to S3 prefix: $S3_PREFIX"
          echo "s3_prefix=$S3_PREFIX" >> $GITHUB_OUTPUT
          
          # Create a file to store the list of uploaded artifacts for the summary
          UPLOADS_LIST=""
          
          # Use the build date from the version action
          BUILD_DATE="${{ steps.version.outputs.build_date }}"
          GIT_VERSION="${{ steps.version.outputs.git_version }}"
          echo "Using build date from version info: $BUILD_DATE"
          echo "Using git version: $GIT_VERSION"
          
          # Process each artifact directory and upload to S3
          for dir in artifacts/bacalhau-*; do
            if [ -d "$dir" ]; then
              OS_ARCH=$(basename "$dir")
          
              # Extract OS and ARCH from directory name 
              if [[ "$OS_ARCH" =~ ^bacalhau-([^-]+)-([^-]+)$ ]]; then
                OS="${BASH_REMATCH[1]}"
                ARCH="${BASH_REMATCH[2]}"
              else
                echo "Skipping invalid directory name: $OS_ARCH"
                continue
              fi
          
              echo "Processing $OS/$ARCH"
          
              # Find artifacts (tarballs and signature files)
              ARTIFACTS=$(find "$dir" -name "*.tar.gz" -o -name "*.tar.gz.sig" -o -name "*.sig" -o -name "*.asc" -o -name "*.signature.sha256")
              if [ -z "$ARTIFACTS" ]; then
                echo "No artifacts found in $dir, skipping"
                continue
              fi
          
              # Process and upload each artifact found
              for ARTIFACT in $ARTIFACTS; do
                FILENAME=$(basename "$ARTIFACT")
          
                # Set metadata for S3 object
                METADATA="BuildDate=$BUILD_DATE,GOOS=$OS,GOARCH=$ARCH,GitCommit=${{ steps.version.outputs.git_commit }},GitVersion=$GIT_VERSION,Major=${{ steps.version.outputs.major }},Minor=${{ steps.version.outputs.minor }}"
          
                # Upload to target-specific path with original filename
                TARGET_DEST="s3://${{ vars.S3_BUCKET }}/versions/$S3_PREFIX/$GIT_VERSION/$FILENAME"
                echo "Uploading to target-specific path: $TARGET_DEST"
                aws s3 cp "$ARTIFACT" "$TARGET_DEST" --metadata "$METADATA"
                UPLOADS_LIST="${UPLOADS_LIST}versions/$S3_PREFIX/$GIT_VERSION/$FILENAME|$OS/$ARCH|$BUILD_DATE\n"
              done
            fi
          done
          
          # Save uploads list for summary
          echo -e "$UPLOADS_LIST" > uploads_list.txt

      - name: Generate and upload release manifest
        id: manifest
        run: |
          S3_PREFIX="${{ steps.upload.outputs.s3_prefix }}"
          GIT_VERSION="${{ steps.version.outputs.git_version }}"
          BUILD_DATE="${{ steps.version.outputs.build_date }}"
          
          echo "Creating manifest for $GIT_VERSION (S3 prefix: $S3_PREFIX)"
          
          # Create JSON manifest using jq
          jq -n \
            --arg version "$GIT_VERSION" \
            --arg buildDate "$BUILD_DATE" \
            --arg gitCommit "${{ steps.version.outputs.git_commit }}" \
            --argjson major ${{ steps.version.outputs.major }} \
            --argjson minor ${{ steps.version.outputs.minor }} \
            '{
              "version": $version,
              "buildDate": $buildDate,
              "gitCommit": $gitCommit,
              "major": $major,
              "minor": $minor,
              "artifacts": {}
            }' > latest-manifest.json
          
          # Add artifact information for each OS/arch combination
          for dir in artifacts/bacalhau-*; do
            if [ -d "$dir" ]; then
              OS_ARCH=$(basename "$dir")
          
              if [[ "$OS_ARCH" =~ ^bacalhau-([^-]+)-([^-]+)$ ]]; then
                OS="${BASH_REMATCH[1]}"
                ARCH="${BASH_REMATCH[2]}"
          
                # Find the tarball for this OS/arch
                TARBALL=$(find "$dir" -name "*.tar.gz" | head -1)
                if [ -n "$TARBALL" ]; then
                  FILENAME=$(basename "$TARBALL")
                  # Create an entry that points to the target-specific path
                  TARGET_PATH="/versions/$S3_PREFIX/$GIT_VERSION/$FILENAME"
                  
                  # Check for signatures
                  SIGNATURES=()
                  SIG_FILES=$(find "$dir" -name "*.tar.gz.sig" -o -name "*.sig" -o -name "*.asc" -o -name "*.signature.sha256")
                  for SIG_FILE in $SIG_FILES; do
                    SIG_FILENAME=$(basename "$SIG_FILE")
                    SIG_PATH="/versions/$S3_PREFIX/$GIT_VERSION/$SIG_FILENAME"
                    SIGNATURES+=("\"$SIG_PATH\"")
                  done
                  
                  # Join signature array into a comma-separated string
                  if [ ${#SIGNATURES[@]} -gt 0 ]; then
                    SIGNATURE_JSON="[$(echo "${SIGNATURES[@]}" | tr ' ' ',')]"
                  else
                    SIGNATURE_JSON="[]"
                  fi
                  
                  # Add to the JSON using jq
                  jq --arg key "${OS}-${ARCH}" \
                     --arg filename "$FILENAME" \
                     --arg path "$TARGET_PATH" \
                     --arg os "$OS" \
                     --arg arch "$ARCH" \
                     --argjson signatures "$SIGNATURE_JSON" \
                     '.artifacts[$key] = {
                       "filename": $filename,
                       "path": $path,
                       "os": $os,
                       "arch": $arch,
                       "signatures": $signatures
                     }' latest-manifest.json > temp.json && mv temp.json latest-manifest.json
                fi
              fi
            fi
          done
          
          # Show the generated manifest for debugging
          echo "Generated manifest:"
          cat latest-manifest.json
          
          # Upload manifest to the manifests directory
          aws s3 cp latest-manifest.json "s3://${{ vars.S3_BUCKET }}/manifests/$S3_PREFIX.json" \
            --metadata "LatestVersion=$GIT_VERSION,BuildDate=$BUILD_DATE" \
            --content-type "application/json"
          echo "Created target-specific manifest at s3://${{ vars.S3_BUCKET }}/manifests/$S3_PREFIX.json"

      - name: Generate upload summary
        if: success()
        run: |
          echo "## S3 Upload Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… Successfully uploaded binaries to S3" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Format metadata as a table for better readability
          echo "| Attribute | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| **Target** | ${{ inputs.target_type }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Release Type** | ${{ steps.version.outputs.release_type }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **S3 Bucket** | ${{ vars.S3_BUCKET }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **S3 Region** | ${{ vars.AWS_REGION }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Git Version** | ${{ steps.version.outputs.git_version }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Git Commit** | ${{ steps.version.outputs.git_commit }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Build Date** | ${{ steps.version.outputs.build_date }} |" >> $GITHUB_STEP_SUMMARY
          
          # Add manifest paths section if applicable
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Release Manifests" >> $GITHUB_STEP_SUMMARY
          echo "| Type | Manifest Path |" >> $GITHUB_STEP_SUMMARY
          echo "|------|--------------|" >> $GITHUB_STEP_SUMMARY
          echo "| ${{ inputs.target_type }} | \`s3://${{ vars.S3_BUCKET }}/manifests/${{ steps.upload.outputs.s3_prefix }}.json\` |" >> $GITHUB_STEP_SUMMARY
          
          # Add uploaded files section
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Uploaded Files" >> $GITHUB_STEP_SUMMARY
          
          # Check if there are any files uploaded
          if [ -s uploads_list.txt ]; then
            # Count how many unique OS/ARCH combinations were uploaded
            PLATFORMS=$(cat uploads_list.txt | cut -d'|' -f2 | sort -u | wc -l)
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Artifacts published for **$PLATFORMS** platforms:" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          
            # Create a simplified markdown table header with only Platform and S3 Path
            echo "| Platform | S3 Path |" >> $GITHUB_STEP_SUMMARY
            echo "|----------|---------|" >> $GITHUB_STEP_SUMMARY
          
            # Sort by platform for better readability and only show platform and S3 path
            sort -t'|' -k2 uploads_list.txt | while IFS='|' read -r S3_PATH PLATFORM BUILD_DATE; do
              # Skip any empty lines that might cause formatting issues
              if [[ -z "$PLATFORM" || -z "$S3_PATH" ]]; then
                continue
              fi
          
              # Add table row with just platform and S3 path
              echo "| $PLATFORM | \`s3://${{ vars.S3_BUCKET }}/$S3_PATH\` |" >> $GITHUB_STEP_SUMMARY
            done
          else
            echo "No files were uploaded." >> $GITHUB_STEP_SUMMARY
          fi