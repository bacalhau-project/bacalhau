name: Publish Binaries to S3

on:
  workflow_call:
    inputs:
      target_type:
        description: 'Target type for publishing (main, nightly, release, non-release)'
        required: true
        type: string
      environment:
        description: 'Environment for publishing (e.g. release)'
        required: false
        default: 'release'
        type: string
    secrets:
      AWS_ACCESS_KEY_ID:
        required: true
      AWS_SECRET_ACCESS_KEY:
        required: true

jobs:
  publish-to-s3:
    name: Publish to S3
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}  # Use the specified environment
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: "0"  # Fetch all history for proper versioning

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: bacalhau-*
          path: artifacts

      - name: List all downloaded artifacts
        run: |
          echo "==== LISTING ALL DOWNLOADED ARTIFACTS ===="
          find artifacts -type f | sort
          echo "==== ARTIFACT DIRECTORIES ===="
          find artifacts -type d | sort
          echo "==== TOTAL FILE COUNT ===="
          find artifacts -type f | wc -l
          echo "==== TOTAL DIRECTORY COUNT ===="
          find artifacts -type d | wc -l

      - name: Get version info
        id: version
        uses: ./.github/actions/get-version-info

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Upload binaries to S3
        id: upload
        run: |
          # Determine S3 prefix based on target type
          case "${{ inputs.target_type }}" in
            main)
              S3_PREFIX="main"
              ;;
            nightly)
              S3_PREFIX="nightly"
              ;;
            release)
              # Handle both release and pre-release the same way for S3 paths
              S3_PREFIX="release"
              ;;
            *)
              echo "Unknown target type: ${{ inputs.target_type }}"
              exit 1
              ;;
          esac
          
          echo "Publishing to S3 prefix: $S3_PREFIX"
          
          # Check if we should also publish to /latest
          PUBLISH_TO_LATEST="false"
          if [[ "${{ steps.version.outputs.release_type }}" == "release" ]]; then
            PUBLISH_TO_LATEST="true"
            echo "Will also publish to /latest prefix (release type: ${{ steps.version.outputs.release_type }})"
          fi
          
          # Create a file to store the list of uploaded artifacts for the summary
          UPLOADS_LIST=""
          
          # Use the build date from the version action
          BUILD_DATE="${{ steps.version.outputs.build_date }}"
          echo "Using build date from version info: $BUILD_DATE"
          
          # Process each artifact directory and upload to S3
          for dir in artifacts/*; do
            if [ -d "$dir" ]; then
              OS_ARCH=$(basename "$dir")
          
              # Extract OS and ARCH from directory name 
              if [[ "$OS_ARCH" =~ ^bacalhau-([^-]+)-([^-]+)$ ]]; then
                OS="${BASH_REMATCH[1]}"
                ARCH="${BASH_REMATCH[2]}"
              else
                echo "Skipping invalid directory name: $OS_ARCH"
                continue
              fi
          
              echo "Processing $OS/$ARCH"
              
              # Find both tarballs and signature files
              ARTIFACTS=$(find "$dir" -name "*.tar.gz" -o -name "*.tar.gz.sig" -o -name "*.sig" -o -name "*.asc")
              if [ -z "$ARTIFACTS" ]; then
                echo "No artifacts found in $dir, skipping"
                continue
              fi
              
              # Process and upload each artifact found
              for ARTIFACT in $ARTIFACTS; do
                FILENAME=$(basename "$ARTIFACT")
                
                # Set metadata for S3 object - always use the build date from version info
                METADATA="BuildDate=$BUILD_DATE,GOOS=$OS,GOARCH=$ARCH,GitCommit=${{ steps.version.outputs.git_commit }},GitVersion=${{ steps.version.outputs.git_version }},Major=${{ steps.version.outputs.major }},Minor=${{ steps.version.outputs.minor }}"
                
                # Always upload to the primary prefix
                echo "Uploading to s3://${{ vars.S3_BUCKET }}/$S3_PREFIX/$FILENAME"
                echo "With metadata: $METADATA"
                aws s3 cp "$ARTIFACT" "s3://${{ vars.S3_BUCKET }}/$S3_PREFIX/$FILENAME" \
                  --metadata "$METADATA"
                
                # Add to summary list with build date and full S3 path
                UPLOADS_LIST="${UPLOADS_LIST}${S3_PREFIX}/${FILENAME}|${OS}/${ARCH}|${BUILD_DATE}\n"
                
                # If it's a full release, also upload to /latest
                if [[ "$PUBLISH_TO_LATEST" == "true" ]]; then
                  echo "Also uploading to s3://${{ vars.S3_BUCKET }}/latest/$FILENAME"
                  aws s3 cp "$ARTIFACT" "s3://${{ vars.S3_BUCKET }}/latest/$FILENAME" \
                    --metadata "$METADATA"
                  
                  # Add the latest upload to the summary as well
                  UPLOADS_LIST="${UPLOADS_LIST}latest/${FILENAME}|${OS}/${ARCH}|${BUILD_DATE}\n"
                fi
              done
            fi
          done
          
          # Save uploads list for summary
          echo -e "$UPLOADS_LIST" > uploads_list.txt

      - name: Generate upload summary
        if: success()
        run: |
          echo "## S3 Upload Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… Successfully uploaded binaries to S3" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Format metadata as a table for better readability
          echo "| Attribute | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| **Target** | ${{ inputs.target_type }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Release Type** | ${{ steps.version.outputs.release_type }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **S3 Bucket** | ${{ vars.S3_BUCKET }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **S3 Region** | ${{ vars.AWS_REGION }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Git Version** | ${{ steps.version.outputs.git_version }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Git Commit** | ${{ steps.version.outputs.git_commit }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Build Date** | ${{ steps.version.outputs.build_date }} |" >> $GITHUB_STEP_SUMMARY
          
          # Add uploaded files section
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Uploaded Files" >> $GITHUB_STEP_SUMMARY
          
          # Check if there are any files uploaded
          if [ -s uploads_list.txt ]; then
            # Count how many unique OS/ARCH combinations were uploaded
            PLATFORMS=$(cat uploads_list.txt | cut -d'|' -f2 | sort -u | wc -l)
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Artifacts published for **$PLATFORMS** platforms:" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          
            # Create a simplified markdown table header with only Platform and S3 Path
            echo "| Platform | S3 Path |" >> $GITHUB_STEP_SUMMARY
            echo "|----------|---------|" >> $GITHUB_STEP_SUMMARY
          
            # Sort by platform for better readability and only show platform and S3 path
            # Ensure no blank rows - make sure each line has proper formatting
            sort -t'|' -k2 uploads_list.txt | while IFS='|' read -r S3_PATH PLATFORM BUILD_DATE; do
              # Skip any empty lines that might cause formatting issues
              if [[ -z "$PLATFORM" || -z "$S3_PATH" ]]; then
                continue
              fi
          
              # Add table row with just platform and S3 path
              echo "| $PLATFORM | \`s3://${{ vars.S3_BUCKET }}/$S3_PATH\` |" >> $GITHUB_STEP_SUMMARY
            done
          else
            echo "No files were uploaded." >> $GITHUB_STEP_SUMMARY
          fi